# 1장 서론
## 1.1 임베딩이란
Embedding(임베딩) : 자연어를 숫자의 나열인 벡터로 바꾼 결과 혹은 그 일련의 과정

### 1.2 임베딩의 역할
> - 단어/문장 간 관련도 계산
> - 의미적/문법적 정보 함축
> - 전이 학습

### 1.2.1. 단어/문장 간 관련도 계산
단어-문서 행렬은 가장 단순한 형태의 임베딩이다. 대표적인 것이 2013년 구글이 발표한 Word2Vec이라는 기법이다.
말그대로 단어를 벡터로 바꾸고 단어별 벡터의 유사도를 측정한 기법이다.
t-SNE라는 차원 축소 기법으로 엄청난 차원의 단어 벡터를 2차원으로 줄여 시각화할 수 있다. 관련성 높은 단어들이 주변에 몰려 있는 것을 확인 할 수 있다.

### 1.2.2. 의미/문법 정보 함축
임베딩은 사칙연상이 가능하다. 단어간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계 도출이 가능하다.
ex) 아들 + 딸 - 아빠 = 엄마, 아들과 딸 더한 값에서 아빠를 뺀 값과 가장 유사도가 높은 값은 엄마이다.

### 1.2.3. 전이 학습
Transfer learning : 특정 문제를 풀기 위해 학습한 모델을 다른 문제를 푸는 데 재사용하는 기법
즉, 텍스트를 벡터로 바꾼 후 이를 입력값으로 하는 모델링을 실시하는 기법이다.

## 1.3. 임베딩 기법의 역사와 종류
### 1.3.1. 통계 기반에서 뉴렬 네트워크 기반으로
신경망 기반 모델들은 이전 단어들이 주어졌을 때 다음 단어가 뭐가 될지 예측하거나 문장 내 일부분을 비워두고 이를 맞추는 과정에서 학습된다. 

### 1.3.2. 단어 수준에서 문장 수준으로
단어 수준의 임베딩 기법은 각 벡터에 해당 단어의 문맥적 의미를 함축한다. 하지만 단점은 동음이의어를 분간하기 어렵다. 문장 수준의 임베딩 기법은 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축하기 때문에 단어 임베딩 기법보다 전이 학습 효과가 좋은 것으로 알려져 있다. 

### 1.3.3. 룰 - 엔드투엔드 - 프리트레인/파인튜닝
다운스트림 태스크란 우리가 풀고 싶은 자연어 처리의 구체적 문제 ex) 품사 판별, 개체명 인식
업스트림 태스크란 다운스트림 태스크에 앞서 해결해야하는 과제라는 뜻이다. ex) 단어/문장 임베딩을 프리트레인하는 작업

### 1.3.4. 임베딩의 종류와 성능
행렬 분해 기반 방법은 말뭉치 정보가 들어 있는 원래 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식의 임베딩 기법. 분해한 후 둘 중 하나의 행렬만 쓰거나 둘을 더하거나 이어 붙여 임베딩에 사용한다. ex) GloVe, Swivel

예측 기반 방법은 어떤 단어 주변에 특정 단어가 나타날지 예측하거나 이전 단어들이 주어 졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 예측하는 과정에서 학습하는 방법이다. 신경망 방법이 여기에 속한다. ex) 단어 수준 : Word2Vec, FastText / 문장 수준 : BERT, XLMo, GPT

토픽 기반 방법은 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 기법도 있다. 잠재 디리클레 할당이 대표적인 기법이다. LDA 같은 모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해할 수 있다. 잠재 의미 분석 - 잠재 디리클레 할당.

## 1.4. 개발 환경
Linux 및 GPU 활용을 위해 AWS EC2 사용하고자 한다. 다만 Limit이 있어 당장은 사용이 어렵다. 인스턴스 할당량 증가 요청을 서비스팀에 제출했으며 답장을 기다리고 있다. 